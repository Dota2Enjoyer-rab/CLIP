The interpretation of language models is the most interesting topic in deep learning spheere. This project was created to demonstrate the advantages and disadvantages of SAE for CLIP.

CLIP - CLIP (Contrastive Languageâ€“Image Pre-training) builds on a large body of work on zero-shot transfer, natural language supervision, and multimodal learning. The idea of zero-data learning dates back over a decade8 but until recently was mostly studied in computer vision as a way of generalizing to unseen object categories.9, 10 A critical insight was to leverage natural language as a flexible prediction space to enable generalization and transfer. In 2013, Richard Socher and co-authors at Stanford11 developed a proof of concept by training a model on CIFAR-10 to make predictions in a word vector embedding space and showed this model could predict two unseen classes. The same year DeVISE12 scaled this approach and demonstrated that it was possible to fine-tune an ImageNet model so that it could generalize to correctly predicting objects outside the original 1000 training set.
